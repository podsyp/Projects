{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My public kernel: https://www.kaggle.com/podsyp/complete-linear-model-guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective for competition: https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n",
    "\n",
    "* Predict the value of homes.\n",
    "* Exploratory data analysis.\n",
    "* Feature engineering.\n",
    "* Create your own regression models.\n",
    "* Compare metrics.\n",
    "* Choose a model.\n",
    "* To predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_summary as ps\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer as Imputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "warnings.simplefilter('ignore')\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We collect the metrics of all the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'data/train.csv' does not exist: b'data/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ecf2274a4f56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfolder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'data/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msub_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'sample_submission.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\des\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\des\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\des\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\des\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\des\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'data/train.csv' does not exist: b'data/train.csv'"
     ]
    }
   ],
   "source": [
    "folder = 'data/'\n",
    "train_df = pd.read_csv(folder+'train.csv')\n",
    "test_df = pd.read_csv(folder+'test.csv')\n",
    "sub_df = pd.read_csv(folder+'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train: ', train_df.shape)\n",
    "print('test: ', test_df.shape)\n",
    "print('sample_submission: ', sub_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = ps.DataFrameSummary(train_df)\n",
    "print('categoricals: ', dfs.categoricals.tolist())\n",
    "print('numerics: ', dfs.numerics.tolist())\n",
    "dfs.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = ps.DataFrameSummary(test_df)\n",
    "print('categoricals: ', dfs.categoricals.tolist())\n",
    "print('numerics: ', dfs.numerics.tolist())\n",
    "dfs.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop('Id', inplace=True, axis=1)\n",
    "test_df.drop('Id', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.DataFrameSummary(train_df[['SalePrice']]).summary().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "sns.distplot(train_df['SalePrice'] , fit=norm);\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(train_df['SalePrice'])\n",
    "\n",
    "#Now plot the distribution\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('SalePrice distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's logarithm the value of the house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['SalePrice'] = np.log(train_df['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.DataFrameSummary(train_df[['SalePrice']]).summary().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "sns.distplot(train_df['SalePrice'] , fit=norm);\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(train_df['SalePrice'])\n",
    "\n",
    "#Now plot the distribution\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('SalePrice distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df['SalePrice']\n",
    "train_df.drop(['SalePrice'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check some Null's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_table(df, info=True):\n",
    "        mis_val = df.isnull().sum()\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        if info:\n",
    "            print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "                \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "                  \" columns that have missing values.\")\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_table(train_df).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_table(test_df).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop columns with a lot of NaNs (more than 80%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_df = missing_values_table(train_df)\n",
    "drops = miss_df[miss_df['% of Total Values'] >80].index.tolist()\n",
    "train_df.drop(drops, inplace=True, axis=1)\n",
    "test_df.drop(drops, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dropped: ', drops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del miss_df\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categoricals & Numerics columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = ps.DataFrameSummary(train_df)\n",
    "cat_cols = dfs.categoricals.tolist() + dfs.bools.tolist()\n",
    "num_cols = dfs.numerics.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing categoricals variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[cat_cols] = train_df[cat_cols].fillna('?')\n",
    "test_df[cat_cols] = test_df[cat_cols].fillna('?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make mean target encoding for categorical feature\n",
    "\n",
    "Let us consider the above table (A simple binary classification). \n",
    "\n",
    "$$ MeanTargetEnc_i = {((GlobalMean * C) + (Mean_i * Size)) \\over (C + Size)} $$\n",
    "\n",
    "Instead of finding the mean of the targets, we can also focus on median and other statistical correlations….These are broadly called target encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEncoding(BaseEstimator):\n",
    "    \"\"\"   In Mean Encoding we take the number \n",
    "    of labels into account along with the target variable \n",
    "    to encode the labels into machine comprehensible values    \"\"\"\n",
    "    \n",
    "    def __init__(self, feature, C=0.1):\n",
    "        self.C = C\n",
    "        self.feature = feature\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        \n",
    "        df = pd.DataFrame({'feature': X_train[self.feature], 'target': y_train}).dropna()\n",
    "        \n",
    "        self.global_mean = df.target.mean()\n",
    "        mean = df.groupby('feature').target.mean()\n",
    "        size = df.groupby('feature').target.size()\n",
    "        \n",
    "        self.encoding = (self.global_mean * self.C + mean * size) / (self.C + size)\n",
    "    \n",
    "    def transform(self, X_test):\n",
    "        \n",
    "        X_test[self.feature] = X_test[self.feature].map(self.encoding).fillna(self.global_mean).values\n",
    "        \n",
    "        return X_test\n",
    "    \n",
    "    def fit_transform(self, X_train, y_train):\n",
    "        \n",
    "        df = pd.DataFrame({'feature': X_train[self.feature], 'target': y_train}).dropna()\n",
    "        \n",
    "        self.global_mean = df.target.mean()\n",
    "        mean = df.groupby('feature').target.mean()\n",
    "        size = df.groupby('feature').target.size()\n",
    "        self.encoding = (self.global_mean * self.C + mean * size) / (self.C + size)\n",
    "        \n",
    "        X_train[self.feature] = X_train[self.feature].map(self.encoding).fillna(self.global_mean).values\n",
    "        \n",
    "        return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in cat_cols:\n",
    "    me = MeanEncoding(f, C=0.1)\n",
    "    me.fit(train_df, y)\n",
    "    train_df = me.transform(train_df)\n",
    "    test_df = me.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing numerics variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputer with median strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(strategy=\"median\")\n",
    "imputer.fit(train_df[num_cols])\n",
    "train_df[num_cols] = imputer.transform(train_df[num_cols])\n",
    "test_df[num_cols] = imputer.transform(test_df[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = ps.DataFrameSummary(train_df)\n",
    "dfs.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = ps.DataFrameSummary(test_df)\n",
    "dfs.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.hist(figsize=(35, 30));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corr = train_df.corr()\n",
    "# plot the heatmap and annotation on it\n",
    "fig, ax = plt.subplots(figsize=(18,18))\n",
    "sns.heatmap(train_corr, xticklabels=train_corr.columns, yticklabels=train_corr.columns, annot=True, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ooops :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_corr = pd.DataFrame(train_corr[(train_corr > 0.8) & (train_corr != 1)].fillna(0).sum(axis=0))\n",
    "high_corr[high_corr[0]>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select columns with high correlation  to drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drops = ['Exterior2nd', '1stFlrSF', 'GrLivArea', 'Fireplaces', 'GarageCars', 'GarageCond', 'SaleType']\n",
    "train_df.drop(drops, inplace=True, axis=1)\n",
    "test_df.drop(drops, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit\n",
    "\n",
    "Regression models are the simplest and at the same time effective machine learning models that give interpretable results.\n",
    "However, using ready-made implementations from popular libraries, such as Scikit-Learn, with little thought about what is happening under the hood, this is one thing.\n",
    "But to implement all the features of these algorithms with your own code is completely different.\n",
    "\n",
    "In this project work, I plan to thoroughly understand the theoretical foundations of regression models (including regularized ones), how to optimize them (analytical solutions, various gradient descent implementations), implement these algorithms myself and compare them with the implementation in Scikit-Learn.\n",
    "\n",
    "In the framework of this work, regression models will be considered that are used to predict continuous random variables (logistic regression is not considered).\n",
    "\n",
    "A large amount of time was devoted more to the engineering task - preparing a dataset with weather data for 6 years, on the basis of which a real customer (agricultural holding) would like to predict yield. Due to the fact that the data are not complete, they had to be enriched with data from open sources.\n",
    "On the part of this data, we will check the models under consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression models without regularization\n",
    "\n",
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "___Linear regression calculates the weighted sum of input values (feature values) and free term .___\n",
    "\n",
    "__The formula for predicting linear regression in the analytical form:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{y}_k = \\theta_0 + \\theta_1 x_{1}^{(k)} + \\theta_2 x_{2}^{(k)} + ... + \\theta_i x_{i}^{(k)} + ... + \\theta_m x_{m}^{(n)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "$\\hat{y}_k$ - calculated value obtained from the $k$ - observation vector using the model;\n",
    "\n",
    "$\\theta_i$ - value of the $i- parameter of the model;\n",
    "\n",
    "$x_{i}^{(k)}$ - the value of the $i$- characteristic of the $k$- vector of the $X$ matrix of size $n$*$(m+1)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___There are 2 types of training for this model:___\n",
    "    \n",
    "- the use of a direct equation in an analytical form that directly calculates the model parameters that are most suitable for a particular data set.\n",
    "- application of the gradient descent method, which iteratively selects the model parameters, reducing the values of the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Everything is clear with the model. How to train it? To teach this means to set her such parameters with which she will most accurately predict the values of the target variable on the training data .___\n",
    "\n",
    "___What does \"predict as accurately as possible\" mean? What measure can be used?___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___There are many ways to measure the deviation of forecast values from real values. It is necessary to somehow evaluate the differences between the forecast and real values (residuals).\n",
    "Here are some of the ways:___\n",
    "    \n",
    "__1. Summarize leftovers__\n",
    "\n",
    "`Advantages`: easy to understand.\n",
    "\n",
    "`Disadvantages`: terms with opposite signs can cancel each other and we will not receive information about the actual size of the deviation.\n",
    "\n",
    "$$\\sum_{k=1}^{n} (\\hat{y}_k - y_k)$$\n",
    "\n",
    "__2. Sum the remainder modules and divide by the number of forecasts__\n",
    "\n",
    "`Advantages`: works if errors are distributed abnormally, easy to calculate.\n",
    "\n",
    "`Disadvantages`: does not work, if the errors are distributed normally, the function of the sum of modules is not differentiable at zero.\n",
    "\n",
    "$$\\sum_{k=1}^n |\\hat{y}_k - y_k|$$\n",
    "\n",
    "_3. Sum the squares (or other positive even degrees) of the residuals and divide by the number of predictions__\n",
    "\n",
    "ʻAdvantages`: it works regardless of the distribution of errors, the sum of squares function is differentiable.\n",
    "\n",
    "`Disadvantages`: it is possible to use only positive even degrees, it is more difficult to calculate than the difference (the absolute value of the difference), in the presence of outliers the error value can“ explode ”, with an error value of 1 it is always 1 (1 to any degree is 1), large degrees are difficult to calculate.\n",
    "\n",
    "$$\\sum_{k=1}^n (\\hat{y}_k - y_k)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The choice of the loss function affects the result of our efforts to select the best parameters (coefficients $\\theta$) .__\n",
    "\n",
    "__As a rule, the value of the sum of losses (squared or modules, it doesn’t matter) is normalized to the number of observations (divided by the number of observations) to obtain the average value.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__We will analyze the most common criteria.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Mean Absolute Error - MAE (Mean Absolute Error):__\n",
    "\n",
    "$$ MAE(X,h_\\theta) = \\frac{1}{n}\\sum_{k=1}^n |\\hat{y}_k - y_k|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Mean Squared Error - MSE (RMS Error):__\n",
    "\n",
    "$$MSE(X,h_\\theta) = \\frac{1}{n}\\sum_{k=1}^n (\\theta^T X_k - y_k)^2 = \\frac{1}{n}\\sum_{k=1}^n (h_\\theta(X_k) - y_k)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Root Mean Squared Error - RMSE (square root of root mean square error):__\n",
    "\n",
    "$$RMSE(X,h_\\theta) = \\sqrt{\\frac{1}{n}\\sum_{k=1}^n (h_\\theta(X_k) - y_k)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__$R^2$ (Share of predicted fluctuations in the total number of fluctuations in the data):__\n",
    "\n",
    "$$R^2 (X,h_\\theta) = 1 - \\frac{\\sum_{k=1}^n (h_\\theta(X_k) - y_k)^2}{\\sum_{k=1}^n (h_\\theta(X_k) - \\bar{y})^2} = 1 - \\frac{RSS}{TSS}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data\n",
    "scaler = StandardScaler().fit(train_df)\n",
    "# train_df = pd.DataFrame(scaler.transform(train_df), columns=train_df.columns)\n",
    "# test_df = pd.DataFrame(scaler.transform(test_df), columns=test_df.columns)\n",
    "train_df = scaler.transform(train_df)\n",
    "test_df = scaler.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_df, y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got some values of the coefficients $\\theta$, but we understand that they are obtained from data in which there is noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a unit column to the left of the observation matrix (theta zero parameter will be multiplied by 1)\n",
    "X_train_with_c = np.c_[np.ones((X_train.shape[0], 1)), X_train] \n",
    "theta_best_analytic = np.linalg.inv(X_train_with_c.T.dot(X_train_with_c)).dot(X_train_with_c.T).dot(y_train)\n",
    "pd.DataFrame(theta_best_analytic).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use our trained model for forecasts. Take the points from the test set and make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_with_c = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "y_pred_simple = X_test_with_c.dot(theta_best_analytic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the result (y_pred) with real data (y_test).\n",
    "\n",
    "We measure the RMSE indicator for our own implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics.append(['analytical_solution', mean_squared_error(y_test, y_pred_simple)])\n",
    "mean_squared_error(y_test, y_pred_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare our implementation of the algorithm with the LinearRegression method implemented in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's measure the RMSE for sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics.append(['sklearn_LR', mean_squared_error(y_test, lr.predict(X_test))])\n",
    "mean_squared_error(y_test, lr.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method (calculating the coefficients by solving the normal equation) has one drawback: with a large number of signs, the computational complexity reaches $O(n^3)$. Therefore, we will not apply this approach on a full weather dataset, where up to 800+ predictors are assumed.\n",
    "\n",
    "Therefore, for real-world problems with hundreds and thousands of features, an approach with iterative optimization of parameters is used - Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the linear regression model for compliance with the criteria of the Gauss-Markov theorem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data model is correctly specified if the data has the following properties:\n",
    "\n",
    "1. All $X_k$ are deterministic and not all are equal to each other (the matrix $X$ is deterministic, it contains real observations - the vectors $X_k$);\n",
    "\n",
    "2. Model errors are not systematic, that is, the mathematical expectation of model errors is $ M [u] = 0 $, the variance of model errors is constant and equal to $Var[u]=\\sigma^2$;\n",
    "\n",
    "3. Errors are uncorrelated, that is, $M[u_i, u_j]=0 $ when $i$ is not equal to $j$ (pairwise correlation is zero).\n",
    "\n",
    "__Then under these conditions the least-squares estimates are optimal in the class of linear unbiased estimates, in other words $\\hat{\\theta}=(X^TX)^{-1}X^Ty$ is the best estimate possible .__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error expectation\n",
    "MU = np.round(np.sum((y_test - lr.predict(X_test)))/y_test.shape[0], 5)\n",
    "MU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error variance\n",
    "VAR = ((y_test - lr.predict(X_test)) - np.mean(y_test - lr.predict(X_test))).mean()\n",
    "VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error variance is not generally constant\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "plt.plot(((y_test - lr.predict(X_test)) - np.mean(y_test - lr.predict(X_test))))\n",
    "fig.suptitle(\"Pic. Error spread relative to their mean value\", \\\n",
    "            fontsize = 14, y = 1.03)\n",
    "ax.axhline(y=MU, color='grey', ls='--');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write a function that will take pairs from the array with errors many times and at the end will calculate the expectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_corr(arr, it=10000):\n",
    "    arr = arr.values.reshape(-1,1)\n",
    "    val_list = []\n",
    "    for iti in range(it):\n",
    "        random_index = list(set(np.random.randint(arr.shape[0], size=2)))\n",
    "        val_list.append(np.sum(arr[random_index, :]))\n",
    "        \n",
    "    return np.mean(val_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_corr((y_test - lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The principle of the gradient descent method is to move along a function from some random point in the direction of the anti-gradient. At the same time, the step size (learning rate) is very important, which, if too large, will not allow us to step to the minimum, and if too small, we will go down to the minimum for a very long time .__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__If the signs have different scales, then the algorithm may converge slowly. We recommend scaling data before applying gradient descent .__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement gradient descent in the case of a function with the number of parameters $\\theta$ $i$>=2, you need to calculate the gradient (the vector of partial derivatives for each parameter $\\theta_i$. The gradient shows how much the function changes (increases or decreases) with a small step over all $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the MSE loss function, the partial derivative with respect to $\\theta_i$ will look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$MSE(X,h_\\theta)_i' = \\frac{d}{d\\theta_i} MSE(X,h_\\theta) = \n",
    "(\\frac{1}{n}\\sum_{k=1}^n (h_\\theta(X_k) - y_k)^2)' =  (\\frac{1}{n}\\sum_{k=1}^n (\\theta^T X_k - y_k)^2)' = \n",
    "\\frac{2}{n}\\sum_{k=1}^n (\\theta^T X_k - y_k) \\sum_{k=1}^m x_{i}^{(k)}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Gradient (matrix of partial derivatives of the loss function for each parameter from $\\theta_0$ to $\\theta_i$) of the loss function $MSE$ will look like this:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "grad_\\theta MSE(X,h_\\theta) = \n",
    "\\left(\\begin{array}{cc} \n",
    "\\frac{d}{d\\theta_0} MSE(X,h_\\theta) \\\\\n",
    "\\frac{d}{d\\theta_1} MSE(X,h_\\theta) \\\\\n",
    "... \\\\\n",
    "\\frac{d}{d\\theta_i} MSE(X,h_\\theta) \\\\\n",
    "... \\\\\n",
    "\\frac{d}{d\\theta_m} MSE(X,h_\\theta)\n",
    "\\end{array}\\right)\n",
    "= \\frac{2}{n} X^T (X  \\theta^T - y)\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm is called __*batch gradient descent*__, because at each step it works with a complete package of training data. For real data sets, such an algorithm will work rather slowly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having calculated the gradient (the direction of the maximum growth of the function), we must take a “step” in the opposite direction (along the anti-gradient).\n",
    "At the first step, from the randomly initialized parameters $\\theta$ we subtract the obtained gradient and update the coefficient values in the vector $\\theta$, at the next step we subtract the last calculated gradient from the most recent vector $\\theta$, and so many, many times. In order to somehow normalize the step, we need to introduce\n",
    "another coefficient is $\\mu$, by which we will multiply the gradient. The $\\mu$ parameter is called the __learning rate__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Calculation of the next step of the gradient descent:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta^{next} = \\theta^{current} - \\mu grad_\\theta MSE(X,h_{\\theta^{current}})$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$\\theta^{current}$ - current value of the coefficient vector $\\theta$;\n",
    "\n",
    "$\\theta^{next}$ - the following value of the coefficient vector $\\theta$;\n",
    "\n",
    "$MSE(X,h_{\\theta^{current}})$ - value $MSE$ for function $h$ with the current set of coefficients $\\theta$, which the observation matrix is passed as an argument $X$;\n",
    "\n",
    "$\\mu$ - learning speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Advantages`: the algorithm allows you to find parameters not only in the case of linear regression (for which you can find these parameters analytically), but also for many other models, including neural networks. This advantage applies to all the options for implementing the gradient descent method listed in this paper.\n",
    "\n",
    "`Disadvantages`: you need to carefully select the learning speed, for better convergence, you need to scale the data before applying the algorithm. Both of the latter drawbacks apply to all the options for implementing the gradient descent method listed in this paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Now we will test this algorithm in practice. We have already created the training data. We write a function for gradient descent and then generate new points and make a forecast .__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(mu, x, y, params, numIterations, lf='MSE', prnt=False):\n",
    "    \"\"\"\n",
    "     The function implements a batch gradient descent algorithm.\n",
    "     mu - learning rate\n",
    "     params - number of parameters, including free parameter\n",
    "     numIterations - number of iterations (int)\n",
    "     prnt - whether or not to print the calculation, if prnt = True, \n",
    "     then at every hundredth step the value of the loss function is displayed\n",
    "     lf - loss function, default 'MSE', you can select 'MAE'\n",
    "    \"\"\"\n",
    "    \n",
    "    n = x.shape[0] #количество наблюдений в выборке\n",
    "    theta = np.ones(params).reshape(params,1) # [ 1.  1. 1.] - начальные значения коэффициентов пусть будут равны 1\n",
    "    x_transpose = x.transpose() # транспонированная матрица x\n",
    "    for iter in range( 0, numIterations ):\n",
    "        hypothesis = np.dot(x, theta) # матричное произведение\n",
    "        loss = hypothesis - y.values.reshape(len(y),1) # значение остатка\n",
    "        \n",
    "        if lf=='MSE':\n",
    "            J = np.sum(loss ** 2) / n  # функция потерь (квадраты)\n",
    "            if prnt and (iter % 10000)==0:\n",
    "                print( \"iter %s | MSE: %.3f\" % (iter, J) )\n",
    "        \n",
    "        elif lf=='MAE':\n",
    "            J = np.sum(abs(loss)) / n  # функция потерь (модули)\n",
    "            if prnt and (iter % 10000)==0:\n",
    "                print( \"iter %s | MAE: %.3f\" % (iter, J) )\n",
    "        \n",
    "        gradient = np.dot(x_transpose, loss) * 2 / n         \n",
    "        theta = theta - mu * gradient  # update\n",
    "    \n",
    "    return (theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# add a unit column to the left of the observation matrix (theta zero parameter will be multiplied by 1)\n",
    "X_train_with_c = np.c_[np.ones((X_train.shape[0], 1)), X_train] \n",
    "# run the gradient descent algorithm\n",
    "\n",
    "theta_best_gd = gradient_descent(0.0001, X_train_with_c, y_train, params=X_train_with_c.shape[1], numIterations=100000, lf='MSE', prnt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# received parameters (gradient descent)\n",
    "pd.DataFrame(np.round(theta_best_gd, 3)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# received parameters (analytical solution)\n",
    "pd.DataFrame(np.round(theta_best_analytic, 3)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The obtained parameters are very far from those that we obtained analytically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take new points from the test sample and apply the coefficients to calculate the function value from them,\n",
    "# obtained by our gradient descent algorithm\n",
    "X_test_with_c = np.c_[np.ones((X_test.shape[0], 1)), X_test] \n",
    "y_pred_gd = X_test_with_c.dot(theta_best_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE is also very close to sklearn and far from our analytical solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics.append(['Gradient_Descent', mean_squared_error(y_test, y_pred_gd)])\n",
    "mean_squared_error(y_test, y_pred_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This modification of the algorithm allows you to bypass the problem that the batch method suffers, namely, the need to work with the entire set of training data.\n",
    "At each step, the stochastic gradient descent selects one random sample from the training set and calculates the gradient only on the basis of this sample.\n",
    "This greatly increases the speed of work, but instead of translating to the minimum of the loss function, we will observe a wandering indicator of the loss function, which will decrease only on average.\n",
    "Having reached a minimum, the algorithm will continue to \"rush\" in its vicinity. The final values of the parameters $\\theta$ will be good, but not optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Advantages`: the algorithm is computationally cheaper than the batch version, quickly gives good results, in the case of an irregular loss function (with many local extrema), the algorithm has good chances to jump out of a local minimum and get to a deeper local minimum or even to a global one.\n",
    "\n",
    "`Disadvantages`: the final answer will not be optimal (the algorithm will not stop even if it gets to a minimum), it will most likely just be\" good \"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One solution to the latter drawback is to gradually decrease the ___learning rate___. This approach is called ___simulated annealing___. A special function responsible for changing the learning speed is called the ___learning schedule___. As with the selection of the learning rate, there is no exact answer on how to set it, so with the rate of decrease in the learning rate you need to be careful not to stop ahead of time or to jump over the global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_schedule(val, p1=100, p2=50):\n",
    "    \"\"\"The function takes parameters as arguments\n",
    "    p1 - parameter for the numerator (default is 100),\n",
    "    p2 - parameter for the denominator (default is 50),\n",
    "    val - value set by user\n",
    "    and returns a simple conversion\n",
    "    return p1/(p2 + val)\n",
    "    \"\"\"\n",
    "    return p1/(p2 + val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def st_gradient_descent(x, y, params, num_epochs, num_iter, lf='MSE', prnt=False):\n",
    "    \"\"\"\n",
    "     The function implements the stochastic gradient descent algorithm.\n",
    "     mu - learning rate\n",
    "     params - number of parameters, including free parameter\n",
    "     num_epochs - number of eras\n",
    "     num_iter - the number of iterations (int) in the era\n",
    "     prnt - print or not calculations every 100 iterations\n",
    "     lf - loss function, default 'MSE', you can select 'MAE'\n",
    "     Without the learning_schedule () function, it won’t work.\n",
    "    \"\"\"\n",
    "    \n",
    "    n = x.shape[0] #количество наблюдений в выборке\n",
    "    theta = np.ones(params).reshape(params,1) # [ 1.  1.  1.] - начальные значения коэффициентов пусть будут равны 1\n",
    "    y = y.values.reshape(-1,1)\n",
    "    lr=0.0001\n",
    "    for epoch in range(num_epochs):\n",
    "        for iteration in range(num_iter):\n",
    "            random_index = np.random.randint(n)\n",
    "            x_rand = x[random_index, :].reshape(1, params)\n",
    "            y_rand = y[random_index, :]\n",
    "\n",
    "            hypothesis = np.dot(x_rand, theta) # матричное произведение\n",
    "            loss = hypothesis - y_rand.reshape(-1,1) # значение остатка\n",
    "\n",
    "            if lf=='MSE':\n",
    "                J = np.sum(loss ** 2) / n  # функция потерь (квадраты)\n",
    "                if prnt and (iteration % 10000)==0:\n",
    "                    print( \"epoch %s | iter %s | MSE: %.3f\" % (epoch, iteration, J) )\n",
    "\n",
    "            elif lf=='MAE':\n",
    "                J = np.sum(abs(loss)) / n  # функция потерь (модули)\n",
    "                if prnt and (iteration % 10000)==0:\n",
    "                    print( \"iter %s | MAE: %.3f\" % (iteration, J) )\n",
    "\n",
    "            gradient = np.dot(x_rand.transpose(), loss) * 2 / n  \n",
    "            lr = learning_schedule(val=100, p1=1, p2=10000)\n",
    "            \n",
    "            theta = theta - lr * gradient  # update\n",
    "    \n",
    "    return (theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our algorithm for 50 epochs with 2000 iterations (only 100,000 times) and measure the execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "theta_best_sgd = st_gradient_descent(X_train_with_c, y_train, params=X_train_with_c.shape[1], num_epochs=50, num_iter=2000, lf='MSE', prnt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# received parameters (SGD)\n",
    "pd.DataFrame(np.round(theta_best_sgd, 3)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Comment:` as time measurement showed, stochastic gradient descent is faster than burst. Both implementations in the above examples updated $\\theta$ parameters 100,000 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_with_c = np.c_[np.ones((X_test.shape[0], 1)), X_test] \n",
    "y_pred_sgd = X_test_with_c.dot(theta_best_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics.append(['Stochastic_Gradient_Descent', mean_squared_error(y_test, y_pred_sgd)])\n",
    "mean_squared_error(y_test, y_pred_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with a similar method from the sklearn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_reg = SGDRegressor(max_iter=100000, penalty=None, eta0=0.1, random_state=42)\n",
    "sgd_reg.fit(X_train_with_c, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Parameters returned by SGDRegressor from the sklearn library:')\n",
    "theta_best_sgd_sklearn = np.array(sgd_reg.coef_)\n",
    "pd.DataFrame(np.round(theta_best_sgd_sklearn, 3)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_with_c = np.c_[np.ones((X_test.shape[0], 1)), X_test] \n",
    "y_pred_sk_sgd = X_test_with_c.dot(theta_best_sgd_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics.append(['Sklearn_Stochastic_Gradient_Descent', mean_squared_error(y_test, y_pred_sk_sgd)])\n",
    "mean_squared_error(y_test, y_pred_sk_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Comment:` The sklearn implementation works much more efficiently in terms of computation speed and accuracy (RMSE parameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This modification of the algorithm combines the features of a batch and stochastic implementation.\n",
    "The mini batch gradient descent calculates gradients on small randomly sampled data samples (__mini-batch__).\n",
    "The larger the package, the less the wandering of the algorithm with respect to the shortest path from the starting point to the local minimum.\n",
    "In general, the mini-batch gradient descent is selected closer to the minimum than the stochastic version.\n",
    "On the other hand, this algorithm is more difficult to get away from local minima, especially when there are a lot of them.\n",
    "Given a well-chosen training schedule, the mini-batch gradient descent method allows you to reach a minimum faster than the batch version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Advantages`: the algorithm is computationally cheaper than the batch version, quickly gives good results, in the case of an irregular loss function (with many local extrema), the algorithm has good chances to jump out of a local minimum and get to a deeper local minimum or even to a global one.\n",
    "\n",
    "`Disadvantages`: the final answer will not be optimal (the algorithm will not stop even if it gets to a minimum), it will most likely just be\" good \"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write our own implementation of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mb_gradient_descent(x, y, mu, params, num_epochs, num_iter, batch_size=0.2, lf='MSE', prnt=False):\n",
    "    \"\"\"\n",
    "    The function implements a mini-batch gradient descent algorithm.\n",
    "    mu - learning rate\n",
    "    params - number of parameters, including free parameter\n",
    "    num_iter - the number of iterations (int) in the era\n",
    "    batch_size - packet size for one iteration\n",
    "    prnt - print or not computation\n",
    "    lf - loss function, default 'MSE', you can select 'MAE'\n",
    "    \"\"\"\n",
    "    \n",
    "    n = x.shape[0] #количество наблюдений в выборке\n",
    "    theta = np.ones(params).reshape(params,1) # [ 1.  1.  1.] - начальные значения коэффициентов пусть будут равны 1\n",
    "    y = y.values.reshape(-1,1)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for iteration in range(num_iter):\n",
    "            random_index = list(set(np.random.randint(200, size=round(n*batch_size)).tolist()))\n",
    "            x_rand = x[random_index, :].reshape(len(random_index), params)\n",
    "            y_rand = y[random_index, :]\n",
    "\n",
    "            hypothesis = np.dot(x_rand, theta) # матричное произведение\n",
    "            loss = hypothesis - y_rand # значение остатка\n",
    "\n",
    "            if lf=='MSE':\n",
    "                J = np.sum(loss ** 2) / n  # функция потерь (квадраты)\n",
    "                if prnt and (iteration % 10000)==0:\n",
    "                    print( \"epoch %s | iter %s | MSE: %.3f\" % (epoch, iteration, J) )\n",
    "\n",
    "            elif lf=='MAE':\n",
    "                J = np.sum(abs(loss)) / n  # функция потерь (модули)\n",
    "                if prnt and (iteration % 10000)==0:\n",
    "                    print( \"iter %s | MAE: %.3f\" % (iteration, J) )\n",
    "\n",
    "            gradient = np.dot(x_rand.transpose(), loss) * 2 / n  \n",
    "            theta = theta - mu * gradient  # update\n",
    "    \n",
    "    return (theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our algorithm for 50 epochs with 2000 iterations (only 100,000 times) and measure the execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "theta_best_mbgs = mb_gradient_descent(X_train_with_c, y_train, mu=0.001, params=X_train_with_c.shape[1], num_epochs=50, num_iter=2000, batch_size=0.2, lf='MSE', prnt=True)\n",
    "pd.DataFrame(np.round(theta_best_mbgs, 3)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_with_c = np.c_[np.ones((X_test.shape[0], 1)), X_test] \n",
    "y_pred_mbgs = X_test_with_c.dot(theta_best_mbgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics.append(['Mini_batch_gradient_descent', mean_squared_error(y_test, y_pred_mbgs)])\n",
    "mean_squared_error(y_test, y_pred_mbgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression models with regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Regularization__ is the imposition of certain restrictions on the model in order to avoid retraining.\n",
    "\n",
    "The simplest example of regularization is to use the 2nd degree of the polynomial when generating polynomial features, but the 2nd.\n",
    "In addition, you can artificially limit the size of the coefficients of the $\\theta$ model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularized regression models include:\n",
    "    \n",
    "* Ridge regression ($L_2$ -regulation)\n",
    "* Lasso regression ($L_1$ regularization)\n",
    "* Elastic network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge-regression ($L_2$ -regularization, Tikhonov regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regularization of the model is realized by adding to the loss function *a regularization term*:\n",
    "\n",
    "$$\\alpha \\sum_{i=1}^m \\theta_i^2$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$\\alpha$ - normalization parameter, determines how much we regularize the model. With $\\alpha=0$, ridge regression becomes just a linear regression. For $\\alpha$ close to 1, the model coefficients (weights) tend to zero;\n",
    "\n",
    "$\\theta_i$ - model parameters from $\\theta_1$ to $\\theta_m$.\n",
    "\n",
    "\n",
    "`Note to expression`: *the regularization term* is used in conjunction with the loss function __only__ when training the model. When checking the model you need to use an irregular measure of errors. This is also true for the other regularized regression models described below.\n",
    "\n",
    "The parameter with the free term $\\theta_0$ is not regularized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ridge regression loss function may have the form:\n",
    "\n",
    "$$ J(\\theta) = MSE(X,h_\\theta) + \\frac{1}{2}\\alpha \\sum_{i=1}^m \\theta_i^2$$\n",
    "\n",
    "$$ J(\\theta) = RMSE(X,h_\\theta) + \\frac{1}{2}\\alpha \\sum_{i=1}^m \\theta_i^2$$\n",
    "\n",
    "$\\alpha$ - normalization parameter, determines how much we regularize the model. With $\\alpha=0$, ridge regression becomes just a linear regression. With $\\alpha$ close to 1, the best possible coefficients (weights) of the model tend to zero.\n",
    "\n",
    "$\\theta_i$ - model parameters from $\\theta_1$ to $\\theta_m$.\n",
    "\n",
    "`Note to the expression`: $\\theta_0$ (parameter with a free term) is not regularized! The sum $\\sum_{i = 1}^m$ starts at 1, not 0.\n",
    "\n",
    "Of the various regularization options, we will use first the formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, the vector of coefficients (weights) of the model is formed into a vector of the form:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\alpha \n",
    "\\left(\\begin{array}{cc} \n",
    "(\\theta_1^2 + \\theta_2^2 + ... + \\theta_m^2)^{1/2} \\\\\n",
    "\\theta_1^2 \\\\\n",
    "\\theta_2^2 \\\\\n",
    "... \\\\\n",
    "\\theta_m\n",
    "\\end{array}\\right) \n",
    "$$ \n",
    "\n",
    "Where:\n",
    "\n",
    "$(\\theta_1^2 + \\theta_2^2 + ... + \\theta_m^2)^{1/2}$ - L2-norm of the coefficient vector from $\\theta_1$ до $\\theta_m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying ridge regression, it is recommended to scale the data (bring them to the same dimension), the model is sensitive to the scale of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regularization example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create data for training the model in the amount of m\n",
    "m=100\n",
    "X = 3 * np.random.rand(m, 1)\n",
    "y = 1 + 0.5 * X + np.random.randn(m, 1) / 1.5\n",
    "# create data to test the model in the amount of m\n",
    "X_new = np.linspace(0, 3, m).reshape(m, 1)\n",
    "y_new =  1 + 0.5 * X_new + np.random.randn(m, 1) / 1.5\n",
    "\n",
    "# prepare 6 graphs for visualizing the forecast with a change in alpha\n",
    "fig = plt.figure(figsize = (14,6))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "# список значений alpha\n",
    "alpha_list = [0, 0.00001, 0.002, 0.5, 0.8, 1]\n",
    "for n,i in zip([ni for ni in range(1,len(alpha_list)+1)], alpha_list):\n",
    "    model = Ridge(i) # sklearn ridge regression model with alpha\n",
    "    poly_features = PolynomialFeatures(degree=18, include_bias=False) # PolynomialFeatures с степенью 18\n",
    "    # expanding data for training with polynomial features\n",
    "    Х_poly_features = poly_features.fit_transform(X)\n",
    "    # we scale advanced data for training\n",
    "    X_poly_features_scaled = StandardScaler().fit_transform(Х_poly_features)\n",
    "    # we scale the target variable for training\n",
    "    y_scaled = StandardScaler().fit_transform(y)\n",
    "    \n",
    "    model.fit(X_poly_features_scaled, y_scaled) # обучаем модель Ridge\n",
    "    \n",
    "    # doing the procedure with the data to check (X_new)\n",
    "    X_new_poly_features = poly_features.fit_transform(X_new) # расширяем\n",
    "    X_new_poly_features_scaled = StandardScaler().fit_transform(X_new_poly_features) # шкалируем\n",
    "\n",
    "    y_pred = model.predict(X_new_poly_features_scaled) # делаем предсказание\n",
    "    \n",
    "    # plot\n",
    "    ax = fig.add_subplot(2,3,n)\n",
    "    # all x and y will be drawn in a scaled version\n",
    "    ax.plot(StandardScaler().fit_transform(X_new), y_pred, \"r\", label='X_new', linewidth=4)\n",
    "    ax.plot(StandardScaler().fit_transform(X_new), StandardScaler().fit_transform(y_new), \"b.\")\n",
    "    ax.set_title(\"alpha={}\".format(i), fontsize = 10)\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.set_ylim((-3, 3))\n",
    "    fig.suptitle(\"Pic. Forecast change with increasing coefficient limit\", \\\n",
    "            fontsize = 14, y = 1.03)\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a unit column to the left of the observation matrix (theta zero parameter will be multiplied by 1)\n",
    "X_train_with_c = np.c_[np.ones((X_train.shape[0], 1)), X_train] \n",
    "theta_best_analytic = np.linalg.inv(X_train_with_c.T.dot(X_train_with_c)).dot(X_train_with_c.T).dot(y_train)\n",
    "pd.DataFrame(theta_best_analytic).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the alpha parameter\n",
    "alpha = 0.5\n",
    "# create matrix A\n",
    "A = np.zeros((theta_best_analytic.shape[0],theta_best_analytic.shape[0]))\n",
    "for s,c in zip([si for si in range(A.shape[0])], [ci for ci in range(A.shape[1])]):\n",
    "    A[s,c] = alpha\n",
    "A[0,0] = 0\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_best_ridge = np.linalg.inv(X_train_with_c.T.dot(X_train_with_c) + alpha*A).dot(X_train_with_c.T).dot(y_train)\n",
    "pd.DataFrame(np.round(theta_best_ridge, 3)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_with_c = np.c_[np.ones((X_test.shape[0], 1)), X_test] \n",
    "pd.DataFrame(X_test_with_c[:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction using our model\n",
    "y_pred_ridge = X_test_with_c.dot(theta_best_ridge)\n",
    "all_metrics.append(['Ridge', mean_squared_error(y_test, y_pred_ridge)])\n",
    "mean_squared_error(y_test, y_pred_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare with the sklearn implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ridge = Ridge(alpha=1)\n",
    "model_ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics.append(['sklearn_Ridge', mean_squared_error(y_test, model_ridge.predict(X_test))])\n",
    "mean_squared_error(y_test, model_ridge.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note:` native and off-the-shelf implementations work with roughly the same RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso regression\n",
    "#### ($L_1$ -regularization, least absolute shrinkage and selection operator regression regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso regression is another type of regularized regression models in which the L1 norm of the weight vector $\\theta$ is used instead of 1/2 the norm square of the weight vector $\\theta$ (as in the case of ridge regression).\n",
    "\n",
    "Lasso regression loss function:\n",
    "\n",
    "$$ J(\\theta) = MSE(X,h_\\theta) + \\alpha \\sum_{i=1}^m |\\theta_i|$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$\\alpha$ - normalization parameter, determines how much we regularize the model. With $\\alpha=0$, the lasso regression becomes just a linear regression. When $\\alpha$ is close to 1, the coefficients (weights) of the model tend to zero (the least important signs are reset first).\n",
    "\n",
    "$\\theta_i$ - model parameters from $\\theta_1$ to $\\theta_m$. Moreover, $\\theta_0$ (parameter with a free term) is not regularized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Advantages`: an important feature of lasso regression is its ability to nullify the coefficients ($\\theta$) for the least important features, i.e. provide a more sparse (with fewer coefficients) model. The value of the coefficient contributes to the loss function rather large (taken modulo); ideally, to minimize such a contribution, it would be nice to reset the coefficients affecting the forecast accuracy very little. In the case of ridge regression, where the contribution of the value of each coefficient to the loss function is the squared coefficient, it is enough to underestimate the coefficient so that it becomes less than 1 (since numbers from 0 to 1 give a smaller number when squared).\n",
    "\n",
    "`Disadvantages`: the lasso-regression loss function is not differentiable at zero, you will have to apply gradient descent and do the trick using a subgradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error function with the addition of a vector-subgradient (which we use at points with x's equal to 0):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "MSE(X,h_\\theta) + \\alpha\n",
    "\\left(\\begin{array}{cc} \n",
    "sign (\\theta_1) \\\\\n",
    "sign (\\theta_2) \\\\\n",
    "... \\\\\n",
    "sign (\\theta_n)\n",
    "\\end{array}\\right) \n",
    "$$ \n",
    "\n",
    "Where:\n",
    "\n",
    "$sign (\\theta_i) = \\begin{cases} \n",
    "\\displaystyle -1,  \\text{если $\\theta_i$ < 0} \\\\\n",
    "\\displaystyle 0,  \\text{если $\\theta_i$ = 0} \\\\\n",
    "\\displaystyle +1, \\text{если $\\theta_i$ > 0}\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(mu, x, y, params, numIterations, lf='MSE', prnt=False):\n",
    "    \"\"\"\n",
    "    The function implements a batch gradient descent algorithm.\n",
    "    mu - learning rate\n",
    "    params - number of parameters, including free parameter\n",
    "    numIterations - number of iterations (int)\n",
    "    prnt - whether or not to print the calculation, if prnt = True, then at every hundredth step the value of the loss function is displayed\n",
    "    lf - loss function, default 'MSE', you can select 'MAE'\n",
    "    \"\"\"\n",
    "    \n",
    "    n = x.shape[0] # number of observations in the sample\n",
    "    theta = np.ones(params).reshape(params,1) # [ 1.  1. 1.] - начальные значения коэффициентов пусть будут равны 1\n",
    "    x_transpose = x.transpose() # transposed matrix x\n",
    "    #print('y', y.shape)\n",
    "    for iter in range( 0, numIterations ):\n",
    "        hypothesis = np.dot(x, theta) # matrix multiplication\n",
    "        loss = hypothesis - y.values.reshape(len(y),1) # residue value\n",
    "        \n",
    "        if lf=='MSE':\n",
    "            J = np.sum(loss ** 2) / n  # loss function (squares)\n",
    "            if prnt and (iter % 10000)==0:\n",
    "                print( \"iter %s | MSE: %.3f\" % (iter, J) )\n",
    "        \n",
    "        elif lf=='MAE':\n",
    "            J = np.sum(abs(loss)) / n  # loss function (modules)\n",
    "            if prnt and (iter % 10000)==0:\n",
    "                print( \"iter %s | MAE: %.3f\" % (iter, J) )\n",
    "        \n",
    "        gradient = np.dot(x_transpose, loss) * 2 / n         \n",
    "        theta = theta - mu * gradient  # update\n",
    "    \n",
    "    return (theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_with_subgrad(mu, x, y, alpha, params, numIterations, prnt=False):\n",
    "    \"\"\"\n",
    "    The function implements a batch gradient descent algorithm using a subgradient with x values equal to 0.\n",
    "    mu - learning rate\n",
    "    alpha - hyperparameter of the Lasso regression loss function\n",
    "    params - number of parameters, including free parameter\n",
    "    numIterations - number of iterations (int)\n",
    "    prnt - whether or not to print the calculation, \n",
    "    if prnt = True, then at every hundredth step the value of the loss function is displayed\n",
    "    \"\"\"\n",
    "    \n",
    "    n = x.shape[0] # number of observations in the sample\n",
    "    theta = np.ones(params).reshape(params,1) # [ 1.  1. 1.] - initial values of the coefficients let be equal to 1\n",
    "    x_transpose = x.transpose() # transposed matrix x\n",
    "    \n",
    "    for iter in range( 0, numIterations ):\n",
    "        hypothesis = np.dot(x, theta) # matrix multiplication\n",
    "        loss = hypothesis - y.values.reshape(-1,1) # residue value\n",
    "        \n",
    "        \n",
    "        J = np.sum(loss ** 2) / n  + alpha*np.sum(np.abs(theta[1:,:]))# loss function (squares)\n",
    "        if prnt and (iter % 10000)==0:\n",
    "            print( \"iter %s | MSE + alpha*sum(abs(theta)): %.3f\" % (iter, J) )\n",
    "        \n",
    "        if np.sum(theta == 0)>0: # if at least one of theta parameters is zero\n",
    "            gradient = np.dot(x_transpose, loss) * 2 / n # the gradient is initially calculated as usual\n",
    "            # but we add one more term to the gradient to circumvent the non-differentiability of the loss function at zeros\n",
    "            term = (gradient > 0)*np.ones(gradient.shape[0]).reshape(gradient.shape[0],1) + \\\n",
    "            (gradient < 0)*np.ones(gradient.shape[0]).reshape(gradient.shape[0],1)*(-1)\n",
    "            # add this term to the gradient\n",
    "            gradient = gradient + alpha*term.reshape(gradient.shape[0], 1)\n",
    "        else:\n",
    "            gradient = np.dot(x_transpose, loss) * 2 / n # in other cases, the gradient is calculated as usual\n",
    "            \n",
    "        theta = theta - mu * gradient  # update\n",
    "    \n",
    "    return (theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# add a unit column to the left of the observation matrix (theta zero parameter will be multiplied by 1)\n",
    "X_train_with_c = np.c_[np.ones((X_train.shape[0], 1)), X_train] \n",
    "# run the gradient descent algorithm\n",
    "theta_best_lasso = gradient_descent_with_subgrad(0.0001, X_train_with_c, y_train, 0.5, params=X_train_with_c.shape[1], numIterations=100000, prnt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.round(theta_best_lasso, 3)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use our trained model for forecasts. Let's make a forecast for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_with_c = np.c_[np.ones((X_test.shape[0], 1)), X_test] \n",
    "pd.DataFrame(X_test_with_c[:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction using our model\n",
    "y_pred_lasso = X_test_with_c.dot(theta_best_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics.append(['Lasso', mean_squared_error(y_test, y_pred_lasso)])\n",
    "mean_squared_error(y_test, y_pred_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to sklearn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_reg = Lasso(alpha=0.5)\n",
    "lasso_reg.fit(X_train, y_train)\n",
    "all_metrics.append(['sklearn_Lasso', mean_squared_error(y_test, lasso_reg.predict(X_test))])\n",
    "mean_squared_error(y_test, lasso_reg.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An elastic net is a combination of ridge and lasso regression.\n",
    "\n",
    "Elastic Net Loss Function:\n",
    "\n",
    "$$ J(\\theta) = MSE(X,h_\\theta) + r\\alpha \\sum_{i=1}^m |\\theta_i| + \\frac{1-r}{2}\\alpha \\sum_{i=1}^m \\theta_i ^2$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$r$ is a hyperparameter that controls the proportion between L2 and L1 regularization.\n",
    "\n",
    "For $r = 0$, an elastic network becomes just a ridge regression, and for $r = 1$ it becomes a lasso regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement our own algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_with_subgrad_for_elastic_net(mu, x, y, alpha, r, params, numIterations, prnt=False):\n",
    "    \"\"\"\n",
    "    The function implements a batch gradient descent algorithm using a subgradient with x values equal to 0.\n",
    "    mu - learning rate\n",
    "    alpha - hyperparameter of the Lasso regression loss function\n",
    "    params - number of parameters, including free parameter\n",
    "    numIterations - number of iterations (int)\n",
    "    prnt - whether or not to print the calculation, if prnt = True, \n",
    "    then at every hundredth step the value of the loss function is displayed\n",
    "    \"\"\"\n",
    "    \n",
    "    n = x.shape[0] # number of observations in the sample\n",
    "    theta = np.ones(params).reshape(params,1) # [ 1.  1. 1.] - initial values of the coefficients let be equal to 1\n",
    "    x_transpose = x.transpose() # transposed matrix x\n",
    "    \n",
    "    for iter in range( 0, numIterations ):\n",
    "        hypothesis = np.dot(x, theta) # matrix multiplication\n",
    "        loss = hypothesis - y.values.reshape(-1,1) # residue value\n",
    "        \n",
    "        J = np.sum(loss ** 2) / n  + r*alpha*np.sum(np.abs(theta[1:,:])) + (1-r)/2 * alpha*np.sum(theta[1:,:]**2)\n",
    "        if prnt and (iter % 10000)==0:\n",
    "            print( \"iter %s | MSE+r*alpha*sum(abs(theta))+(1-r)/2*alpha(theta^2): %.3f\" % (iter, J) )\n",
    "        \n",
    "        if np.sum(theta == 0)>0: # if at least one of theta parameters is zero\n",
    "            gradient = np.dot(x_transpose, loss) * 2 / n # the gradient is initially calculated as usual\n",
    "            # but we add one more term to the gradient to circumvent the non-differentiability of the loss function at zeros\n",
    "            term = (gradient > 0)*np.ones(gradient.shape[0]).reshape(gradient.shape[0],1) + \\\n",
    "            (gradient < 0)*np.ones(gradient.shape[0]).reshape(gradient.shape[0],1)*(-1)\n",
    "            # add this term to the gradient\n",
    "            gradient = gradient + alpha*term.reshape(gradient.shape[0], 1)\n",
    "        else:\n",
    "            gradient = np.dot(x_transpose, loss) * 2 / n # in other cases, the gradient is calculated as usual\n",
    "            \n",
    "        theta = theta - mu * gradient  # update\n",
    "    \n",
    "    return (theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# add a unit column to the left of the observation matrix (theta zero parameter will be multiplied by 1)\n",
    "X_train_with_c = np.c_[np.ones((X_train.shape[0], 1)), X_train] \n",
    "# run the gradient descent algorithm\n",
    "theta_best_enet = gradient_descent_with_subgrad_for_elastic_net(0.0001, X_train_with_c, y_train, 0.5, 0.5, params=X_train_with_c.shape[1], numIterations=100000, prnt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.round(theta_best_enet, 3)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take test data\n",
    "# add left column vector with units\n",
    "X_test_with_c = np.c_[np.ones((X_test.shape[0], 1)), X_test] \n",
    "pd.DataFrame(X_test_with_c[:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction using our model\n",
    "y_pred_elnet = X_test_with_c.dot(theta_best_enet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics.append(['Elastic_net', mean_squared_error(y_test, y_pred_elnet)])\n",
    "mean_squared_error(y_test, y_pred_elnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_net = ElasticNet(alpha=0.5, l1_ratio=0.5, random_state=42)\n",
    "elastic_net.fit(X_train, y_train)\n",
    "all_metrics.append(['sklearn_Elastic_net', mean_squared_error(y_test, elastic_net.predict(X_test))])\n",
    "mean_squared_error(y_test, elastic_net.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Achieving work goals:\n",
    "\n",
    "1. The study of the most famous regression models used for forecasting (comparing results, determining the advantages and disadvantages of models).\n",
    "\n",
    "2. Collection and preparation of real weather data (predictors) and house price data (target variable) for study.\n",
    "\n",
    "3. The use of regression models on real data to predict the yield of house prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementation of design work tasks:\n",
    "\n",
    "To study and put into practice the following regression models:\n",
    "\n",
    "* Linear regression. __Performed.__\n",
    "* Ridge regression. __Performed.__\n",
    "* Lasso regression. __Performed.__\n",
    "* Elastic network. __Performed.__\n",
    "\n",
    "To study and apply optimization methods:\n",
    "\n",
    "* Analytical solutions. __Performed.__\n",
    "* Gradient descent:\n",
    "* batch. __Performed.__\n",
    "* stochastic. __Performed.__\n",
    "* mini batch. __Performed.__\n",
    "    \n",
    "\n",
    "Collect and prepare weather data, solve the problem of comparability (identical information in different sources is indicated by different names, often measurements are made in different units) and incomplete data (data are not available for long periods) in the following ways:\n",
    "\n",
    "* Processing of data provided by the customer (work with missing values, aggregation of indicators by periods). __Performed.__\n",
    "* Enrichment of weather data with data obtained by the API from open sources. __Performed.__\n",
    "* Designing signs that operate on different phases of the price of houses. __Performed.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose the best algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(all_metrics, columns=['Algo', 'RMSE']).sort_values('RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use our own ridge regression algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_with_c = np.c_[np.ones((test_df.shape[0], 1)), test_df] \n",
    "pd.DataFrame(test_df_with_c[:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_predict = test_df_with_c.dot(theta_best_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(log_predict)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df['SalePrice'] = np.round(np.exp(log_predict), 0)\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
