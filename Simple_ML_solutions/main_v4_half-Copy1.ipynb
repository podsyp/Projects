{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Half and Half\n",
    "This notebook is the Python implementation of this awesomely simple R code: https://www.kaggle.com/kailex/ac-dc by [kxx](https://www.kaggle.com/kailex)\n",
    "\n",
    "It demonstrates splitting the data in half and using each half to build a model which performs very well on the public LB with minimal feature engineering. The discussion on the same: https://www.kaggle.com/c/ashrae-energy-prediction/discussion/115851"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\n",
    "import hyperopt.pyll\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import KernelPCA, PCA\n",
    "\n",
    "path_data = \"data/\"\n",
    "path_train = path_data + \"train.csv\"\n",
    "path_test = path_data + \"test.csv\"\n",
    "path_building = path_data + \"building_metadata.csv\"\n",
    "path_weather_train = path_data + \"weather_train.csv\"\n",
    "path_weather_test = path_data + \"weather_test.csv\"\n",
    "\n",
    "plt.style.use(\"seaborn\")\n",
    "sns.set(font_scale=1)\n",
    "\n",
    "myfavouritenumber = 0\n",
    "seed = myfavouritenumber\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading train data\n",
    "Reading train data along with building and weather metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(path_train)\n",
    "\n",
    "building = pd.read_csv(path_building)\n",
    "\n",
    "weather_train = pd.read_csv(path_weather_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.query('not (building_id <= 104 & meter == 0 & timestamp <= \"2016-05-20\")')\n",
    "le = LabelEncoder()\n",
    "building.primary_use = le.fit_transform(building.primary_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train = df_train.median().astype(np.float16)\n",
    "#df_train['building_median'] = df_train['building_id'].map(building_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "## Memory optimization\n",
    "\n",
    "# Original code from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage by @gemartin\n",
    "# Modified to support timestamp type, categorical type\n",
    "# Modified to add option to use float16\n",
    "\n",
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "\n",
    "def reduce_mem_usage(df, use_float16=False):\n",
    "    \"\"\"\n",
    "    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    \n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
    "            continue\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
    "    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 757.98 MB\n",
      "Memory usage after optimization is: 322.52 MB\n",
      "Decreased by 57.4%\n",
      "Memory usage of dataframe is 0.06 MB\n",
      "Memory usage after optimization is: 0.02 MB\n",
      "Decreased by 72.6%\n",
      "Memory usage of dataframe is 9.60 MB\n",
      "Memory usage after optimization is: 2.65 MB\n",
      "Decreased by 72.4%\n"
     ]
    }
   ],
   "source": [
    "df_train = reduce_mem_usage(df_train, use_float16=True)\n",
    "building = reduce_mem_usage(building, use_float16=True)\n",
    "weather_train = reduce_mem_usage(weather_train, use_float16=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data\n",
    "There are two files with features that need to be merged with the data. One is building metadata that has information on the buildings and the other is weather data that has information on the weather.   \n",
    "\n",
    "Note that the only features created are hour, weekday and is_holiday!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(X, building_data, weather_data, test=False):\n",
    "    \"\"\"\n",
    "    Preparing final dataset with all features.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = X.merge(building_data, on=\"building_id\", how=\"left\")\n",
    "    X = X.merge(weather_data, on=[\"site_id\", \"timestamp\"], how=\"left\")\n",
    "    \n",
    "    X.timestamp = pd.to_datetime(X.timestamp, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    X.square_feet = np.log1p(X.square_feet)\n",
    "    \n",
    "    if not test:\n",
    "        X.sort_values(\"timestamp\", inplace=True)\n",
    "        X.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "\n",
    "    if test:\n",
    "        row_ids = X.row_id\n",
    "        X.drop(\"row_id\", axis=1, inplace=True)\n",
    "        return X, row_ids\n",
    "    else:\n",
    "        y = np.log1p(X.meter_reading)\n",
    "        X.drop(\"meter_reading\", axis=1, inplace=True)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays = [\"2016-01-01\", \"2016-01-18\", \"2016-02-15\", \"2016-05-30\", \"2016-07-04\",\n",
    "                \"2016-09-05\", \"2016-10-10\", \"2016-11-11\", \"2016-11-24\", \"2016-12-26\",\n",
    "                \"2017-01-01\", \"2017-01-16\", \"2017-02-20\", \"2017-05-29\", \"2017-07-04\",\n",
    "                \"2017-09-04\", \"2017-10-09\", \"2017-11-10\", \"2017-11-23\", \"2017-12-25\",\n",
    "                \"2018-01-01\", \"2018-01-15\", \"2018-02-19\", \"2018-05-28\", \"2018-07-04\",\n",
    "                \"2018-09-03\", \"2018-10-08\", \"2018-11-12\", \"2018-11-22\", \"2018-12-25\",\n",
    "                \"2019-01-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_timestamp(X, holidays):\n",
    "    X[\"hour\"] = X.timestamp.dt.hour\n",
    "    X[\"weekday\"] = X.timestamp.dt.weekday\n",
    "    X[\"is_holiday\"] = (X.timestamp.dt.date.astype(\"str\").isin(holidays)).astype(int)\n",
    "    X[\"is_holiday\"] = X[[\"is_holiday\", \"weekday\"]].apply(lambda x: 1 if x['is_holiday'] == 1 or x['weekday'] in (5, 6) else 0, axis=1)\n",
    "       \n",
    "    #drop_features = [\"timestamp\",]\n",
    "    drop_features = [\"timestamp\", \"sea_level_pressure\", \"wind_direction\", \"wind_speed\"]\n",
    "\n",
    "    X.drop(drop_features, axis=1, inplace=True)\n",
    "    gc.collect()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'drop_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-db24d2c8a280>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuilding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweather_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweather_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-6be55d4b57c3>\u001b[0m in \u001b[0;36mprepare_data\u001b[1;34m(X, building_data, weather_data, test)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m#drop_features = [\"timestamp\", \"sea_level_pressure\", \"wind_direction\", \"wind_speed\"]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'drop_features' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, y_train = prepare_data(df_train, building, weather_train)\n",
    "X_train = prepare_data(X_train, holidays)\n",
    "\n",
    "del df_train, weather_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_features = [\"sea_level_pressure\", \"wind_direction\", \"wind_speed\"]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[PCA_features].fillna(0))\n",
    "pca_scaled_train = scaler.transform(X_train[PCA_features].fillna(0))\n",
    "#pca = KernelPCA(n_components=1, kernel='poly', n_jobs=-1)\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(pca_scaled_train)\n",
    "X_train['pca_fet'] = pca.transform(pca_scaled_train)\n",
    "X_train.drop(PCA_features, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-fold LightGBM Model split half-and-half\n",
    "The data is split into two based on time. Each half is used as the training data for a model.\n",
    "\n",
    "**Half 1:** The first 50% rows of train data   \n",
    "**Half 2:** The last 50% rows of train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with first half and validating on second half:\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 0.871236\tvalid_1's rmse: 1.14431\n",
      "[400]\ttraining's rmse: 0.824784\tvalid_1's rmse: 1.13874\n",
      "[600]\ttraining's rmse: 0.803417\tvalid_1's rmse: 1.137\n",
      "[800]\ttraining's rmse: 0.781466\tvalid_1's rmse: 1.13803\n",
      "Early stopping, best iteration is:\n",
      "[622]\ttraining's rmse: 0.800507\tvalid_1's rmse: 1.13652\n",
      "Building model with second half and validating on first half:\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 0.888524\tvalid_1's rmse: 1.15284\n",
      "[400]\ttraining's rmse: 0.843645\tvalid_1's rmse: 1.14677\n",
      "[600]\ttraining's rmse: 0.821549\tvalid_1's rmse: 1.14653\n",
      "Early stopping, best iteration is:\n",
      "[524]\ttraining's rmse: 0.830506\tvalid_1's rmse: 1.14603\n"
     ]
    }
   ],
   "source": [
    "X_half_1 = X_train[:int(X_train.shape[0] / 2)]\n",
    "X_half_2 = X_train[int(X_train.shape[0] / 2):]\n",
    "\n",
    "y_half_1 = y_train[:int(X_train.shape[0] / 2)]\n",
    "y_half_2 = y_train[int(X_train.shape[0] / 2):]\n",
    "\n",
    "cat = [\"building_id\", \"site_id\", \"meter\", \"primary_use\", \"hour\", \"weekday\"]\n",
    "\n",
    "d_half_1 = lgb.Dataset(X_half_1, label=y_half_1, categorical_feature=cat, free_raw_data=False)\n",
    "d_half_2 = lgb.Dataset(X_half_2, label=y_half_2, categorical_feature=cat, free_raw_data=False)\n",
    "\n",
    "watchlist_1 = [d_half_1, d_half_2]\n",
    "watchlist_2 = [d_half_2, d_half_1]\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"num_leaves\": 45,\n",
    "    \"learning_rate\": 0.055,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"reg_lambda\": 2,\n",
    "    \"metric\": \"rmse\"\n",
    "}\n",
    "\n",
    "print(\"Building model with first half and validating on second half:\")\n",
    "model_half_1 = lgb.train(params, train_set=d_half_1, num_boost_round=1200, valid_sets=watchlist_1, verbose_eval=200, early_stopping_rounds=200)\n",
    "\n",
    "print(\"Building model with second half and validating on first half:\")\n",
    "model_half_2 = lgb.train(params, train_set=d_half_2, num_boost_round=1200, valid_sets=watchlist_2, verbose_eval=200, early_stopping_rounds=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.8, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.8, 'lambda_l1': 2.75, 'lambda_l2': 5.25, 'learning_rate': 0.045, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 62, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7332333446061745, auc test 0.7444556017784256, dif 0.000758801790547569, \n",
      "\t final score 0.7452144035689732 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.9, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.6000000000000001, 'lambda_l1': 0.5, 'lambda_l2': 6.25, 'learning_rate': 0.05, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 77, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7479232750175413, auc test 0.7582648315509237, dif 0.0006657902894330268, \n",
      "\t final score 0.7589306218403568 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.30000000000000004, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.6000000000000001, 'lambda_l1': 2.75, 'lambda_l2': 7.75, 'learning_rate': 0.05, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 89, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7360539846063205, auc test 0.7477319074887081, dif 0.0008086953850603726, \n",
      "\t final score 0.7485406028737684 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.6000000000000001, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.7000000000000001, 'lambda_l1': 0.0, 'lambda_l2': 8.0, 'learning_rate': 0.05, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 67, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7418118669692478, auc test 0.7520861467287908, dif 0.0006588737711112608, \n",
      "\t final score 0.7527450204999021 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.8, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.8, 'lambda_l1': 1.25, 'lambda_l2': 0.25, 'learning_rate': 0.05, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 55, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7333750595640584, auc test 0.7450511205196739, dif 0.0008084890936396325, \n",
      "\t final score 0.7458596096133135 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.4, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.7000000000000001, 'lambda_l1': 2.75, 'lambda_l2': 3.5, 'learning_rate': 0.045, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 55, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7638189638992756, auc test 0.7719737164248749, dif 0.00045525765546712815, \n",
      "\t final score 0.7724289740803421 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.8, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.6000000000000001, 'lambda_l1': 3.0, 'lambda_l2': 3.25, 'learning_rate': 0.05, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 39, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.8015307860822681, auc test 0.8073141074479876, dif 0.0002627143050172605, \n",
      "\t final score 0.8075768217530048 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.8, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.4, 'lambda_l1': 0.75, 'lambda_l2': 6.75, 'learning_rate': 0.045, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 75, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.8139039758664288, auc test 0.819746275882182, dif 0.0002670140833891364, \n",
      "\t final score 0.8200132899655711 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.9, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.4, 'lambda_l1': 3.75, 'lambda_l2': 7.0, 'learning_rate': 0.06, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 25, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.8824465442890812, auc test 0.8851746221772107, dif 7.89533977966081e-05, \n",
      "\t final score 0.8852535755750073 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.9, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.4, 'lambda_l1': 1.75, 'lambda_l2': 2.25, 'learning_rate': 0.05, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 91, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7941291329247675, auc test 0.8016971445369488, dif 0.00040399023117016007, \n",
      "\t final score 0.8021011347681191 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.4, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.5, 'lambda_l1': 2.5, 'lambda_l2': 1.25, 'learning_rate': 0.05, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 58, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7832131327246329, auc test 0.7904726351255071, dif 0.0003779644600826877, \n",
      "\t final score 0.7908505995855898 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.6000000000000001, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.8, 'lambda_l1': 1.5, 'lambda_l2': 4.0, 'learning_rate': 0.045, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 62, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7328517698744808, auc test 0.7444202956851184, dif 0.0007966083086514609, \n",
      "\t final score 0.7452169039937698 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.6000000000000001, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.9, 'lambda_l1': 0.5, 'lambda_l2': 1.25, 'learning_rate': 0.04, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 54, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7429597486974032, auc test 0.7533948393728712, dif 0.0006754511781516693, \n",
      "\t final score 0.7540702905510229 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.4, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.7000000000000001, 'lambda_l1': 3.25, 'lambda_l2': 8.0, 'learning_rate': 0.055, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 80, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7257561777158371, auc test 0.738478276386988, dif 0.0009274574953073944, \n",
      "\t final score 0.7394057338822955 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.7000000000000001, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.4, 'lambda_l1': 3.0, 'lambda_l2': 2.0, 'learning_rate': 0.055, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 55, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.823060640746491, auc test 0.8283042596345372, dif 0.00022460002003597175, \n",
      "\t final score 0.8285288596545732 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.5, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.8, 'lambda_l1': 2.75, 'lambda_l2': 4.0, 'learning_rate': 0.045, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 83, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7112913156059646, auc test 0.7255431768372658, dif 0.0011122298232564187, \n",
      "\t final score 0.7266554066605222 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.7000000000000001, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.4, 'lambda_l1': 0.75, 'lambda_l2': 5.0, 'learning_rate': 0.05, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 35, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.8664396145713864, auc test 0.869716117649877, dif 0.00010584172968191398, \n",
      "\t final score 0.8698219593795589 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.4, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.8, 'lambda_l1': 2.75, 'lambda_l2': 4.75, 'learning_rate': 0.045, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 29, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7937923324239535, auc test 0.7996314005807825, dif 0.00026677779052519196, \n",
      "\t final score 0.7998981783713077 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.5, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.4, 'lambda_l1': 3.25, 'lambda_l2': 4.0, 'learning_rate': 0.045, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 24, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.9042568728264971, auc test 0.9064193728552051, dif 5.44414639531161e-05, \n",
      "\t final score 0.9064738143191583 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.5, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.7000000000000001, 'lambda_l1': 2.5, 'lambda_l2': 3.75, 'learning_rate': 0.045, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 35, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.8024856155217859, auc test 0.8079919324051458, dif 0.00024287226422120567, \n",
      "\t final score 0.808234804669367 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bagging_fraction': 0.30000000000000004, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.9, 'lambda_l1': 4.0, 'lambda_l2': 6.0, 'learning_rate': 0.055, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 99, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.6836167146589975, auc test 0.7041561135562618, dif 0.0019959073024541554, \n",
      "\t final score 0.706152020858716 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.5, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.9, 'lambda_l1': 4.0, 'lambda_l2': 6.0, 'learning_rate': 0.055, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 97, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.6851062689097075, auc test 0.7052767785718471, dif 0.0019388624295480434, \n",
      "\t final score 0.7072156410013951 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.5, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.9, 'lambda_l1': 4.0, 'lambda_l2': 6.0, 'learning_rate': 0.06, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 98, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.6797035605665882, auc test 0.7011285453877688, dif 0.002135368523329062, \n",
      "\t final score 0.7032639139110979 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.30000000000000004, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.9, 'lambda_l1': 3.75, 'lambda_l2': 5.75, 'learning_rate': 0.06, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 99, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.6778790644299795, auc test 0.6998519270692718, dif 0.0022234052420747265, \n",
      "\t final score 0.7020753323113466 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.30000000000000004, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.9, 'lambda_l1': 3.75, 'lambda_l2': 7.0, 'learning_rate': 0.06, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 93, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.682878246150155, auc test 0.703570987116659, dif 0.0020198021678864827, \n",
      "\t final score 0.7055907892845454 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.30000000000000004, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.5, 'lambda_l1': 3.5, 'lambda_l2': 5.5, 'learning_rate': 0.06, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 45, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7944101129521323, auc test 0.8007019386048776, dif 0.0003006371790061589, \n",
      "\t final score 0.8010025757838838 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.7000000000000001, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.30000000000000004, 'lambda_l1': 2.25, 'lambda_l2': 7.5, 'learning_rate': 0.06, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 71, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.8285388379950135, auc test 0.8341533711968475, dif 0.0002505542922456517, \n",
      "\t final score 0.8344039254890931 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.4, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.9, 'lambda_l1': 3.5, 'lambda_l2': 4.5, 'learning_rate': 0.06, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 84, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.6879651909810266, auc test 0.7075886686912032, dif 0.0018554171313144364, \n",
      "\t final score 0.7094440858225176 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.5, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.8, 'lambda_l1': 4.0, 'lambda_l2': 5.5, 'learning_rate': 0.055, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 99, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.690503897435731, auc test 0.70877126441786, dif 0.0016545562093049193, \n",
      "\t final score 0.7104258206271649 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.30000000000000004, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.9, 'lambda_l1': 2.0, 'lambda_l2': 6.5, 'learning_rate': 0.04, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 87, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7087683182457448, auc test 0.7234266224160089, dif 0.0011634131465029435, \n",
      "\t final score 0.7245900355625118 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.4, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.8, 'lambda_l1': 3.75, 'lambda_l2': 5.5, 'learning_rate': 0.06, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 93, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.6897597368939715, auc test 0.7085457423129925, dif 0.0017303545897201249, \n",
      "\t final score 0.7102760969027127 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.6000000000000001, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.5, 'lambda_l1': 3.5, 'lambda_l2': 7.25, 'learning_rate': 0.055, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 76, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.75807896033597, auc test 0.7675150730523822, dif 0.0005750015189963141, \n",
      "\t final score 0.7680900745713786 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.7000000000000001, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.7000000000000001, 'lambda_l1': 2.25, 'lambda_l2': 3.0, 'learning_rate': 0.06, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 68, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7290813163473677, auc test 0.7412949391458331, dif 0.000868862803461044, \n",
      "\t final score 0.7421638019492941 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.30000000000000004, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.6000000000000001, 'lambda_l1': 3.25, 'lambda_l2': 6.25, 'learning_rate': 0.055, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 96, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.726049562056943, auc test 0.7392393332422617, dif 0.0009826064677342752, \n",
      "\t final score 0.740221939709996 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.5, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.8, 'lambda_l1': 3.75, 'lambda_l2': 4.5, 'learning_rate': 0.06, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 88, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.6933932304495384, auc test 0.7116115534286541, dif 0.0016474545243479884, \n",
      "\t final score 0.7132590079530021 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.6000000000000001, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.9, 'lambda_l1': 1.25, 'lambda_l2': 6.0, 'learning_rate': 0.06, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 79, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.6906981240560347, auc test 0.7096956105918769, dif 0.0017616264914040562, \n",
      "\t final score 0.711457237083281 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.4, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.6000000000000001, 'lambda_l1': 0.0, 'lambda_l2': 7.75, 'learning_rate': 0.055, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 74, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7452046402331206, auc test 0.7560472578115863, dif 0.0007181491067212689, \n",
      "\t final score 0.7567654069183076 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.30000000000000004, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.7000000000000001, 'lambda_l1': 3.0, 'lambda_l2': 5.25, 'learning_rate': 0.06, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 48, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.755340781505324, auc test 0.7642503319935924, dif 0.0005245287159504377, \n",
      "\t final score 0.7647748607095428 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.5, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.8, 'lambda_l1': 4.0, 'lambda_l2': 2.75, 'learning_rate': 0.055, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 66, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7171469247028534, auc test 0.7309175402182243, dif 0.001052750276784474, \n",
      "\t final score 0.7319702904950087 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.4, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.30000000000000004, 'lambda_l1': 1.75, 'lambda_l2': 6.5, 'learning_rate': 0.04, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 90, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.8358204416277695, auc test 0.8408584163749246, dif 0.00021067333875924766, \n",
      "\t final score 0.8410690897136839 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bagging_fraction': 0.8, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.9, 'lambda_l1': 3.5, 'lambda_l2': 0.25, 'learning_rate': 0.06, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 84, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.6856217632162651, auc test 0.7059599049653786, dif 0.0019647080826380803, \n",
      "\t final score 0.7079246130480167 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.7000000000000001, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.7000000000000001, 'lambda_l1': 2.5, 'lambda_l2': 7.0, 'learning_rate': 0.05, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 62, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.748946937707062, auc test 0.7585589132722902, dif 0.0005922434318475853, \n",
      "\t final score 0.7591511567041378 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.6000000000000001, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.8, 'lambda_l1': 3.0, 'lambda_l2': 4.5, 'learning_rate': 0.06, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 95, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.6880558587216737, auc test 0.7075006134163476, dif 0.0018284536252589645, \n",
      "\t final score 0.7093290670416066 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.5, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.7000000000000001, 'lambda_l1': 0.25, 'lambda_l2': 7.5, 'learning_rate': 0.055, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 70, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7340675500335648, auc test 0.7457435855537722, dif 0.0008084862756705548, \n",
      "\t final score 0.7465520718294427 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.4, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.9, 'lambda_l1': 1.0, 'lambda_l2': 3.5, 'learning_rate': 0.05, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 20, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.8103091775844069, auc test 0.8153647540406035, dif 0.00021185225558066926, \n",
      "\t final score 0.8155766062961841 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.8, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.6000000000000001, 'lambda_l1': 3.75, 'lambda_l2': 5.75, 'learning_rate': 0.06, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 50, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7668785540430193, auc test 0.7750094419081272, dif 0.0004531278491409292, \n",
      "\t final score 0.7754625697572681 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.30000000000000004, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.5, 'lambda_l1': 3.25, 'lambda_l2': 5.0, 'learning_rate': 0.055, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 79, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7540032169017011, auc test 0.7641310977235166, dif 0.0006439167635517328, \n",
      "\t final score 0.7647750144870683 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.9, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.8, 'lambda_l1': 1.5, 'lambda_l2': 2.25, 'learning_rate': 0.05, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 44, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7526374985504644, auc test 0.7619365916092824, dif 0.0005617006806735177, \n",
      "\t final score 0.7624982922899559 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.6000000000000001, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.9, 'lambda_l1': 2.25, 'lambda_l2': 1.5, 'learning_rate': 0.06, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 87, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.6819564688905201, auc test 0.7030578334665393, dif 0.0020839959260003367, \n",
      "\t final score 0.7051418293925396 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.4, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.7000000000000001, 'lambda_l1': 3.0, 'lambda_l2': 6.5, 'learning_rate': 0.055, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 59, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7462271153458158, auc test 0.7561007015791975, dif 0.0006182439535867846, \n",
      "\t final score 0.7567189455327843 \n",
      "\n",
      "\n",
      "100%|███████| 50/50 [12:13:48<00:00, 880.58s/it, best loss: 0.7020753323113466]\n",
      "{'bagging_fraction': 0.30000000000000004, 'feature_fraction': 0.9, 'lambda_l1': 3.75, 'lambda_l2': 5.75, 'learning_rate': 0.06, 'num_leaves': 99.38501245513}\n",
      "Wall time: 12h 13min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Чем больше P, тем меньше мы хотим штрафовать за разницу между train  и test\n",
    "p = 0.8\n",
    "# k - количество итераций\n",
    "k = 50\n",
    "\n",
    "skf = KFold(n_splits=3, shuffle=True, random_state=7)\n",
    "\n",
    "def score(params):\n",
    "    print('Training with params:')\n",
    "    print(params)\n",
    "    w=[]\n",
    "    best_iter = []\n",
    "    \n",
    "\n",
    "    for train_index, val_index in skf.split(X_half_1, y_half_1):\n",
    "        x_train_1, x_valid_1 = X_half_1.iloc[train_index, :], X_half_1.iloc[val_index, :]\n",
    "        y_train_1, y_valid_1 = y_half_1.iloc[train_index], y_half_1.iloc[val_index]\n",
    "        train_data = lgb.Dataset(x_train_1, label=y_train_1, categorical_feature=cat)\n",
    "        val_data = lgb.Dataset(x_valid_1, label=y_valid_1, categorical_feature=cat, reference=train_data)\n",
    "        gbm = lgb.train(params,\n",
    "                        train_data,\n",
    "                        valid_sets = [train_data, val_data],\n",
    "                        valid_names=['train', 'val'],\n",
    "                        num_boost_round = 2000,\n",
    "                        verbose_eval = False, \n",
    "                        categorical_feature=cat\n",
    "                       )\n",
    "        w.append([gbm.best_score['train']['rmse'], gbm.best_score['val']['rmse']])\n",
    "        best_iter.append(gbm.best_iteration)\n",
    "    nrounds = np.mean(best_iter)\n",
    "    print('best iter:', int(nrounds), 'all iter:', best_iter)\n",
    "    res = list(np.mean(w, axis=0))\n",
    "    print(\"\\t auc train {0}, auc test {1}, dif {2}, \\n\\t final score {3} \\n\\n\".format(res[0], res[1], np.power(np.square(res[0]-res[1]), p), +res[1]+np.power(np.square(res[0]-res[1]), p)))\n",
    "    return {'loss': +res[1]+np.power(np.square(res[0]-res[1]), p), 'status': STATUS_OK, \n",
    "            'mean_auc_train': res[0], 'mean_auc_test': res[1], 'best_iter': int(nrounds)}\n",
    "\n",
    "def optimize(trials):\n",
    "    space = {\n",
    "        #'max_depth': hp.choice('max_depth', [-1, 6, 7]),\n",
    "        'max_depth': -1,\n",
    "        'num_leaves': scope.int(hp.uniform('num_leaves', 20, 100)),\n",
    "        'lambda_l1': hp.quniform('lambda_l1', 0, 4, 0.25),\n",
    "        'lambda_l2': hp.quniform('lambda_l2', 0, 8, 0.25),\n",
    "        'learning_rate': hp.quniform('learning_rate', 0.04 , 0.06, 0.005),\n",
    "        'feature_fraction': hp.quniform('feature_fraction', 0.3, 0.9, 0.1),\n",
    "        'bagging_fraction': hp.quniform('bagging_fraction', 0.3, 0.9, 0.1),\n",
    "        'metric': ('rmse',),\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'nthread': 8,\n",
    "        'early_stopping_rounds': 10,\n",
    "        'silent':1,\n",
    "    }\n",
    "    best = fmin(score, space, algo=tpe.suggest, trials=trials, max_evals=k)\n",
    "    print(best)\n",
    "\n",
    "trials = Trials()\n",
    "optimize(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.8, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.4, 'lambda_l1': 2.25, 'lambda_l2': 4.0, 'learning_rate': 0.05, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 79, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.8011899758492532, auc test 0.8083020038241756, dif 0.0003657543752701834, \n",
      "\t final score 0.8086677581994458 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.4, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.4, 'lambda_l1': 3.25, 'lambda_l2': 5.5, 'learning_rate': 0.045, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 96, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7939383066583813, auc test 0.8014617017876312, dif 0.0004001862794139366, \n",
      "\t final score 0.8018618880670452 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.8, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.7000000000000001, 'lambda_l1': 0.5, 'lambda_l2': 5.25, 'learning_rate': 0.06, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 89, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.723637719224104, auc test 0.7408598968552385, dif 0.0015057085026146974, \n",
      "\t final score 0.7423656053578531 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.7000000000000001, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.8, 'lambda_l1': 1.25, 'lambda_l2': 0.25, 'learning_rate': 0.045, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 84, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7229168849415446, auc test 0.7392418600068323, dif 0.0013821782116723597, \n",
      "\t final score 0.7406240382185048 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.30000000000000004, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.4, 'lambda_l1': 2.75, 'lambda_l2': 0.0, 'learning_rate': 0.05, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 86, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7954927967765112, auc test 0.803179267684171, dif 0.0004141552589726599, \n",
      "\t final score 0.8035934229431437 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.6000000000000001, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.7000000000000001, 'lambda_l1': 0.0, 'lambda_l2': 4.25, 'learning_rate': 0.055, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 45, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7772211833194747, auc test 0.7858031624848323, dif 0.0004940147651735407, \n",
      "\t final score 0.7862971772500058 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.6000000000000001, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.8, 'lambda_l1': 0.5, 'lambda_l2': 5.0, 'learning_rate': 0.05, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 98, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7062775671415821, auc test 0.7259288017565243, dif 0.0018596180165727028, \n",
      "\t final score 0.727788419773097 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.6000000000000001, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.4, 'lambda_l1': 1.0, 'lambda_l2': 1.5, 'learning_rate': 0.055, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 88, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.789150557708345, auc test 0.7976563107324437, dif 0.0004870128598809386, \n",
      "\t final score 0.7981433235923246 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.7000000000000001, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.4, 'lambda_l1': 0.75, 'lambda_l2': 5.75, 'learning_rate': 0.055, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 59, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.8144995746947149, auc test 0.8204123085086715, dif 0.0002721832056024861, \n",
      "\t final score 0.820684491714274 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.6000000000000001, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.5, 'lambda_l1': 3.0, 'lambda_l2': 2.5, 'learning_rate': 0.055, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 38, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.8112131499426094, auc test 0.8167475545868202, dif 0.00024485752455461943, \n",
      "\t final score 0.8169924121113749 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.6000000000000001, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.5, 'lambda_l1': 3.0, 'lambda_l2': 4.0, 'learning_rate': 0.045, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 47, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.8087774145065795, auc test 0.814617786113522, dif 0.000266873081025238, \n",
      "\t final score 0.8148846591945472 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.7000000000000001, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.8, 'lambda_l1': 1.25, 'lambda_l2': 0.75, 'learning_rate': 0.05, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 76, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7228128216973936, auc test 0.73901179041763, dif 0.001365148156296709, \n",
      "\t final score 0.7403769385739267 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.30000000000000004, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.5, 'lambda_l1': 1.0, 'lambda_l2': 5.0, 'learning_rate': 0.05, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 77, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7672829988320077, auc test 0.7773857875056892, dif 0.0006413661424145172, \n",
      "\t final score 0.7780271536481037 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.9, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.8, 'lambda_l1': 4.0, 'lambda_l2': 5.5, 'learning_rate': 0.055, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 82, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7130351759446233, auc test 0.7304196394674118, dif 0.0015284740469543305, \n",
      "\t final score 0.7319481135143662 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.4, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.6000000000000001, 'lambda_l1': 0.75, 'lambda_l2': 2.75, 'learning_rate': 0.055, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 92, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7369131941683414, auc test 0.7518779642651449, dif 0.0012025747438904355, \n",
      "\t final score 0.7530805390090353 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.6000000000000001, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.5, 'lambda_l1': 0.75, 'lambda_l2': 3.75, 'learning_rate': 0.06, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 46, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7907790184332182, auc test 0.7980697621709907, dif 0.00038057033592286954, \n",
      "\t final score 0.7984503325069136 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.6000000000000001, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.8, 'lambda_l1': 0.25, 'lambda_l2': 5.5, 'learning_rate': 0.06, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 79, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7089115165971821, auc test 0.7275324796550735, dif 0.0017060958305291677, \n",
      "\t final score 0.7292385754856027 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.6000000000000001, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.8, 'lambda_l1': 3.5, 'lambda_l2': 3.75, 'learning_rate': 0.04, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 25, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.822591751122414, auc test 0.8269907666784992, dif 0.00016957772179918872, \n",
      "\t final score 0.8271603444002984 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.5, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.6000000000000001, 'lambda_l1': 3.25, 'lambda_l2': 4.75, 'learning_rate': 0.05, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 50, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7846513199193793, auc test 0.7922545934331158, dif 0.0004070061596772382, \n",
      "\t final score 0.792661599592793 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.5, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.6000000000000001, 'lambda_l1': 3.5, 'lambda_l2': 2.0, 'learning_rate': 0.05, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 83, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.747577926681415, auc test 0.7598664783545094, dif 0.0008774070507541828, \n",
      "\t final score 0.7607438854052636 \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.5, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.9, 'lambda_l1': 0.0, 'lambda_l2': 7.5, 'learning_rate': 0.04, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 99, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7073730740047582, auc test 0.7253227448127514, dif 0.0016087567459665345, \n",
      "\t final score 0.7269315015587179 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.5, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.9, 'lambda_l1': 1.75, 'lambda_l2': 7.5, 'learning_rate': 0.04, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 66, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7354441618580486, auc test 0.7480532145134188, dif 0.0009143067654225209, \n",
      "\t final score 0.7489675212788413 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.4, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.9, 'lambda_l1': 0.0, 'lambda_l2': 7.5, 'learning_rate': 0.045, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 98, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7023603443962413, auc test 0.7219523358416245, dif 0.0018506561380554577, \n",
      "\t final score 0.72380299197968 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.4, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.9, 'lambda_l1': 0.0, 'lambda_l2': 7.75, 'learning_rate': 0.04, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 67, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7331223686011983, auc test 0.7458337033308923, dif 0.0009262022855563906, \n",
      "\t final score 0.7467599056164487 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.4, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.9, 'lambda_l1': 1.75, 'lambda_l2': 6.5, 'learning_rate': 0.045, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 98, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7025145038105927, auc test 0.721513636750747, dif 0.001761870770022912, \n",
      "\t final score 0.7232755075207699 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.30000000000000004, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.7000000000000001, 'lambda_l1': 2.0, 'lambda_l2': 6.5, 'learning_rate': 0.045, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 69, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7582084978676197, auc test 0.7687519362786941, dif 0.0006867072376934602, \n",
      "\t final score 0.7694386435163876 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.4, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.9, 'lambda_l1': 2.5, 'lambda_l2': 6.75, 'learning_rate': 0.045, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 22, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.8094079035877071, auc test 0.8141146964636437, dif 0.00018895576205356138, \n",
      "\t final score 0.8143036522256972 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.30000000000000004, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.9, 'lambda_l1': 1.5, 'lambda_l2': 6.5, 'learning_rate': 0.045, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 99, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7015990603631823, auc test 0.7209490744240553, dif 0.0018142204411220433, \n",
      "\t final score 0.7227632948651773 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.30000000000000004, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.7000000000000001, 'lambda_l1': 1.75, 'lambda_l2': 6.5, 'learning_rate': 0.04, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 94, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7451975699351574, auc test 0.7579524963054577, dif 0.0009312895493874012, \n",
      "\t final score 0.7588837858548451 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.30000000000000004, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.9, 'lambda_l1': 2.25, 'lambda_l2': 7.0, 'learning_rate': 0.045, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 55, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7400751213407212, auc test 0.7517255230231826, dif 0.0008056481956126645, \n",
      "\t final score 0.7525311712187953 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.4, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.7000000000000001, 'lambda_l1': 1.5, 'lambda_l2': 6.0, 'learning_rate': 0.045, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 33, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.8141708983843648, auc test 0.8193426229231062, dif 0.00021969320065861217, \n",
      "\t final score 0.8195623161237648 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.30000000000000004, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.30000000000000004, 'lambda_l1': 2.25, 'lambda_l2': 6.25, 'learning_rate': 0.04, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 74, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.8446558951354067, auc test 0.8489746601071952, dif 0.00016465514755429543, \n",
      "\t final score 0.8491393152547495 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.5, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.8, 'lambda_l1': 1.75, 'lambda_l2': 7.0, 'learning_rate': 0.045, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 92, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7167903517657125, auc test 0.7332789697350304, dif 0.0014044128978249922, \n",
      "\t final score 0.7346833826328554 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.4, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.9, 'lambda_l1': 1.5, 'lambda_l2': 8.0, 'learning_rate': 0.045, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 61, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7327252172221991, auc test 0.7454889603483436, dif 0.000932319759038023, \n",
      "\t final score 0.7464212801073816 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.9, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.7000000000000001, 'lambda_l1': 2.5, 'lambda_l2': 4.5, 'learning_rate': 0.04, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 89, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7479884525929439, auc test 0.760142958284097, dif 0.0008621437458659914, \n",
      "\t final score 0.761005102029963 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.8, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.8, 'lambda_l1': 2.0, 'lambda_l2': 6.0, 'learning_rate': 0.045, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 95, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.716174377485066, auc test 0.7330577920816647, dif 0.0014586008305637402, \n",
      "\t final score 0.7345163929122284 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.30000000000000004, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.30000000000000004, 'lambda_l1': 1.5, 'lambda_l2': 3.0, 'learning_rate': 0.05, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 99, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.8133924303368412, auc test 0.8200070750773766, dif 0.00032569451033859627, \n",
      "\t final score 0.8203327695877152 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.5, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.9, 'lambda_l1': 1.25, 'lambda_l2': 3.5, 'learning_rate': 0.045, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 84, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7122125522783431, auc test 0.7293061680114294, dif 0.0014877648669140374, \n",
      "\t final score 0.7307939328783435 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.4, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.7000000000000001, 'lambda_l1': 2.5, 'lambda_l2': 8.0, 'learning_rate': 0.04, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 72, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.762005708983356, auc test 0.7719820690280216, dif 0.0006285725089863418, \n",
      "\t final score 0.7726106415370079 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.30000000000000004, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.8, 'lambda_l1': 1.0, 'lambda_l2': 7.0, 'learning_rate': 0.05, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 35, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7761705464530254, auc test 0.7836344331570436, dif 0.000395133691328594, \n",
      "\t final score 0.7840295668483722 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bagging_fraction': 0.8, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.9, 'lambda_l1': 0.5, 'lambda_l2': 5.0, 'learning_rate': 0.045, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 60, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.731615149232475, auc test 0.7445369859698023, dif 0.0009508648629723891, \n",
      "\t final score 0.7454878508327747 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.7000000000000001, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.7000000000000001, 'lambda_l1': 2.75, 'lambda_l2': 4.5, 'learning_rate': 0.04, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 81, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7527497026886018, auc test 0.7638925266445886, dif 0.0007502265521924156, \n",
      "\t final score 0.764642753196781 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.30000000000000004, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.8, 'lambda_l1': 1.25, 'lambda_l2': 1.25, 'learning_rate': 0.05, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 86, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7127554894785226, auc test 0.7308729307668081, dif 0.0016328826712763464, \n",
      "\t final score 0.7325058134380845 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.4, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.6000000000000001, 'lambda_l1': 2.25, 'lambda_l2': 5.75, 'learning_rate': 0.045, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 90, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7499401262352521, auc test 0.7619557222761694, dif 0.0008464328512224619, \n",
      "\t final score 0.7628021551273918 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.4, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.4, 'lambda_l1': 1.0, 'lambda_l2': 5.5, 'learning_rate': 0.05, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 54, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.8258916700199074, auc test 0.8309787969544367, dif 0.00021397159248328056, \n",
      "\t final score 0.8311927685469199 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.5, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.5, 'lambda_l1': 0.25, 'lambda_l2': 3.25, 'learning_rate': 0.045, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 64, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7860041243554815, auc test 0.7940670195371687, dif 0.0004470803912851386, \n",
      "\t final score 0.7945140999284539 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.7000000000000001, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.8, 'lambda_l1': 2.75, 'lambda_l2': 4.25, 'learning_rate': 0.04, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 95, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7229147202349706, auc test 0.7383480702555274, dif 0.001263386783640179, \n",
      "\t final score 0.7396114570391675 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.30000000000000004, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.9, 'lambda_l1': 4.0, 'lambda_l2': 5.25, 'learning_rate': 0.055, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 42, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7440602907127457, auc test 0.7546405461588486, dif 0.0006905479578266718, \n",
      "\t final score 0.7553310941166752 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.30000000000000004, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.8, 'lambda_l1': 0.5, 'lambda_l2': 2.25, 'learning_rate': 0.05, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 72, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.7254333994130858, auc test 0.7408059209595175, dif 0.001255429058396617, \n",
      "\t final score 0.7420613500179141 \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'bagging_fraction': 0.5, 'boosting_type': 'gbdt', 'early_stopping_rounds': 10, 'feature_fraction': 0.6000000000000001, 'lambda_l1': 2.0, 'lambda_l2': 7.25, 'learning_rate': 0.045, 'max_depth': -1, 'metric': ('rmse',), 'nthread': 8, 'num_leaves': 27, 'objective': 'regression', 'silent': 1}\n",
      "best iter:                                                                     \n",
      "2000                                                                           \n",
      "all iter:                                                                      \n",
      "[2000, 2000, 2000]                                                             \n",
      "\t auc train 0.8406399793702158, auc test 0.8443049797589076, dif 0.0001266247030968692, \n",
      "\t final score 0.8444316044620045 \n",
      "\n",
      "\n",
      "100%|███████| 50/50 [12:40:42<00:00, 912.86s/it, best loss: 0.7227632948651773]\n",
      "{'bagging_fraction': 0.30000000000000004, 'feature_fraction': 0.9, 'lambda_l1': 1.5, 'lambda_l2': 6.5, 'learning_rate': 0.045, 'num_leaves': 99.72510777872655}\n",
      "Wall time: 12h 40min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Чем больше P, тем меньше мы хотим штрафовать за разницу между train  и test\n",
    "p = 0.8\n",
    "# k - количество итераций\n",
    "k = 50\n",
    "\n",
    "skf = KFold(n_splits=3, shuffle=True, random_state=7)\n",
    "\n",
    "def score(params):\n",
    "    print('Training with params:')\n",
    "    print(params)\n",
    "    w=[]\n",
    "    best_iter = []\n",
    "    \n",
    "\n",
    "    for train_index, val_index in skf.split(X_half_2, y_half_2):\n",
    "        x_train_1, x_valid_1 = X_half_2.iloc[train_index, :], X_half_2.iloc[val_index, :]\n",
    "        y_train_1, y_valid_1 = y_half_2.iloc[train_index], y_half_2.iloc[val_index]\n",
    "        train_data = lgb.Dataset(x_train_1, label=y_train_1, categorical_feature=cat)\n",
    "        val_data = lgb.Dataset(x_valid_1, label=y_valid_1, categorical_feature=cat, reference=train_data)\n",
    "        gbm = lgb.train(params,\n",
    "                        train_data,\n",
    "                        valid_sets = [train_data, val_data],\n",
    "                        valid_names=['train', 'val'],\n",
    "                        num_boost_round = 2000,\n",
    "                        verbose_eval = False, \n",
    "                        categorical_feature=cat\n",
    "                       )\n",
    "        w.append([gbm.best_score['train']['rmse'], gbm.best_score['val']['rmse']])\n",
    "        best_iter.append(gbm.best_iteration)\n",
    "    nrounds = np.mean(best_iter)\n",
    "    print('best iter:', int(nrounds), 'all iter:', best_iter)\n",
    "    res = list(np.mean(w, axis=0))\n",
    "    print(\"\\t auc train {0}, auc test {1}, dif {2}, \\n\\t final score {3} \\n\\n\".format(res[0], res[1], np.power(np.square(res[0]-res[1]), p), +res[1]+np.power(np.square(res[0]-res[1]), p)))\n",
    "    return {'loss': +res[1]+np.power(np.square(res[0]-res[1]), p), 'status': STATUS_OK, \n",
    "            'mean_auc_train': res[0], 'mean_auc_test': res[1], 'best_iter': int(nrounds)}\n",
    "\n",
    "def optimize(trials):\n",
    "    space = {\n",
    "        #'max_depth': hp.choice('max_depth', [-1, 6, 7]),\n",
    "        'max_depth': -1,\n",
    "        'num_leaves': scope.int(hp.uniform('num_leaves', 20, 100)),\n",
    "        'lambda_l1': hp.quniform('lambda_l1', 0, 4, 0.25),\n",
    "        'lambda_l2': hp.quniform('lambda_l2', 0, 8, 0.25),\n",
    "        'learning_rate': hp.quniform('learning_rate', 0.04 , 0.06, 0.005),\n",
    "        'feature_fraction': hp.quniform('feature_fraction', 0.3, 0.9, 0.1),\n",
    "        'bagging_fraction': hp.quniform('bagging_fraction', 0.3, 0.9, 0.1),\n",
    "        'metric': ('rmse',),\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'nthread': 8,\n",
    "        'early_stopping_rounds': 10,\n",
    "        'silent':1,\n",
    "    }\n",
    "    best = fmin(score, space, algo=tpe.suggest, trials=trials, max_evals=k)\n",
    "    print(best)\n",
    "\n",
    "trials = Trials()\n",
    "optimize(trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "Plotting the feature importance from LGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_fimp_1 = pd.DataFrame()\n",
    "df_fimp_1[\"feature\"] = X_train.columns.values\n",
    "df_fimp_1[\"importance\"] = model_half_1.feature_importance()\n",
    "df_fimp_1[\"half\"] = 1\n",
    "\n",
    "df_fimp_2 = pd.DataFrame()\n",
    "df_fimp_2[\"feature\"] = X_train.columns.values\n",
    "df_fimp_2[\"importance\"] = model_half_2.feature_importance()\n",
    "df_fimp_2[\"half\"] = 2\n",
    "\n",
    "df_fimp = pd.concat([df_fimp_1, df_fimp_2], axis=0)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=df_fimp.sort_values(by=\"importance\", ascending=False))\n",
    "plt.title(\"LightGBM Feature Importance\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4660"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X_train, y_train, X_half_1, X_half_2, y_half_1, y_half_2, d_half_1, d_half_2, watchlist_1, watchlist_2, df_fimp_1, df_fimp_2, df_fimp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing test data\n",
    "Preparing test data with same features as train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1272.51 MB\n",
      "Memory usage after optimization is: 358.65 MB\n",
      "Decreased by 71.8%\n",
      "Memory usage of dataframe is 19.04 MB\n",
      "Memory usage after optimization is: 8.96 MB\n",
      "Decreased by 53.0%\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv(path_test)\n",
    "weather_test = pd.read_csv(path_weather_test)\n",
    "\n",
    "df_test = reduce_mem_usage(df_test)\n",
    "weather_test = reduce_mem_usage(weather_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test['building_median'] = X_test['building_id'].map(building_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, row_ids = prepare_data(df_test, building, weather_test, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_scaled_test = scaler.transform(X_test[PCA_features].fillna(0))\n",
    "X_test['pca_fet'] = pca.transform(pca_scaled_test)\n",
    "X_test.drop(PCA_features, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_test, building, weather_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring test data\n",
    "Averaging predictions from the two half train data models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = np.expm1(model_half_1.predict(X_test, num_iteration=model_half_1.best_iteration)) / 2\n",
    "\n",
    "del model_half_1\n",
    "gc.collect()\n",
    "\n",
    "pred += np.expm1(model_half_2.predict(X_test, num_iteration=model_half_2.best_iteration)) / 2\n",
    "    \n",
    "del model_half_2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "Preparing final file for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"row_id\": row_ids, \"meter_reading\": np.clip(pred, 0, a_max=None)})\n",
    "submission.to_csv(\"sub_v4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**P.S.** If you vote up this kernel, please don't forget to vote up the original R version: https://www.kaggle.com/kailex/ac-dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're interested in trying out site specific models, check out my [Divide and Conquer notebook](https://www.kaggle.com/rohanrao/ashrae-divide-and-conquer/output)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
